{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a295ea5",
   "metadata": {},
   "source": [
    "# Codebook of botanical RGCN project\n",
    "\n",
    "by Qianqian Hiris Gu (qianqian.gu@nhm.ac.uk)\n",
    "Natural History Museum, London\n",
    "\n",
    "### Content\n",
    "- Intro/Readme\n",
    "- Data Acquisition and Preprocessing\n",
    "- Heterogeneous Implementation of Relational Graph Convolution Network \n",
    "- Methodology\n",
    "- Homogeneous Implementation of Relational Graph Convolution Network "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5c1c3e",
   "metadata": {},
   "source": [
    "### Intro/Readme\n",
    "\n",
    "    Knowledge graphs have become increasingly important in biodiversity research as they effectively represent, integrate, and explore complex, heterogeneous data. They help integrate diverse data sources, enabling the discovery of new insights and improving the performance of machine learning models. \n",
    "\n",
    "    Graph Convolution Networks (GCNs) have gained significant attention in recent years for their ability to effectively analyse and learn from graph-structured data. As an extension of GCNs, Relational Graph Convolution Networks (R-GCNs) are specifically designed to handle relational data in knowledge graphs. They can model complex relationships between entities and attributes within the graph, making them particularly useful for tasks such as link prediction, entity classification, and knowledge base completion in the digitisation pipeline.\n",
    "\n",
    "    Some of the key features of R-GCNs include the following:\n",
    "\n",
    "    - The ability to handle multi-relational data with various types of relationships.\n",
    "    - Weight sharing across relations to reduce the number of parameters and avoid overfitting.\n",
    "    - Aggregating information from neighbouring nodes with different types of relations.\n",
    "\n",
    "    As we continue to explore the power of graph-based machine learning techniques, the R-GCN family will undoubtedly play a crucial role in advancing our understanding and ability to leverage complex relational data. R-GCNs have shown great potential in various applications, including natural language processing, recommendation systems, and species discovery. Simultaneously, R-GCNs could be improved and fine-tuned by playing some math tricks, for example, calculating attention (RGAT) or linear feature adjustment (GNN-Film).\n",
    "\n",
    "    This Jupyter notebook provides insights into Python functions and models designed for the Planetary Knowledge Base project. Please don't hesitate to reach out if you want more info or to discuss potential re-coding/applications. I'd be happy to share additional resources and engage in a fruitful discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d769a38",
   "metadata": {},
   "source": [
    "### Data Acquisition and Preprocessing\n",
    "\n",
    "    A graph data structure comprises nodes and edges, which defines a graph in data structures and mathematics. We generally use graphs for relationship modelling in complex scenarios. When we use a graph structure to model relationships of real-world entities, nodes can be any item ID that occupies one point of the binary relationship in a homogeneous or heterogeneous graph (no matter how complex the group relationship is, it can be decomposed into combinations of various relationships between pairs). This involves the concepts of homogeneous and heterogeneous graphs. A graph with more than two types of nodes and edges is called a heterogeneous graph, and the edges in the graph are used to represent relationships between two items in the actual modelling process.\n",
    "\n",
    "#### Botanical knowledge graph is heterogeneous.\n",
    "\n",
    "    In this section, my code imports required Python libraries and loads a sample CSV Data for a quick demo. The demo in this document is a featureless approach, but we can always assign feature values to either nodes or edges. For example, if we wanna give a dummy one-hot feature to a node/entity type named \"catalogNumber\", we can use codes like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4e0a5668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# onehot_catalogNumber = pd.get_dummies(data.catalogNumber, prefix='catalogNumber', dummy_na=True)\n",
    "# node_features = onehot_catalogNumber \n",
    "# graph.nodes['catalogNumber'].data['feature'] = node_features\n",
    "# might need to adjust tensor size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43faef23",
   "metadata": {},
   "source": [
    "    We have attached a list of entities and relations of botanical specimens to provide a better understanding of GBIF data that will be used for constructing the knowledge graph.  We've also attached the raw data from Data portal and the Wikidat dump.\n",
    "\n",
    "#### GBIF_nodes_and_relations (herbarium sheet)\n",
    "https://docs.google.com/spreadsheets/d/1S4Ve2A6oy1i0iohAHZ96FgINTt1FDy4PGsVhhx43mmE/edit?usp=sharing\n",
    "\n",
    "#### Unprocessed raw india dataset on Github\n",
    "https://github.com/NaturalHistoryMuseum/rel-graph-data/blob/5194e27a3a28846dae16b62a8aec52974202f80b/india.csv.zip\n",
    "\n",
    "#### Unprocessed Wikidata dump on Github\n",
    "https://github.com/NaturalHistoryMuseum/rel-graph-data/blob/5194e27a3a28846dae16b62a8aec52974202f80b/wdump-2600.nt.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d2d4214b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3289"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import dgl\n",
    "import torch.optim as optim\n",
    "from dgl.dataloading import MultiLayerFullNeighborSampler, EdgeDataLoader\n",
    "from dgl.dataloading.negative_sampler import Uniform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import os\n",
    "import tqdm\n",
    "from dgl import save_graphs, load_graphs\n",
    "import dgl.function as fn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch import GraphConv, SAGEConv, HeteroGraphConv\n",
    "from dgl.utils import expand_as_pair\n",
    "from collections import defaultdict\n",
    "import torch as th\n",
    "import dgl.nn as dglnn\n",
    "from dgl.data.utils import makedirs, save_info, load_info\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d055591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder to convert entity to numeric value\n",
    "def encode_map(input_array):\n",
    "    p_map={}\n",
    "    length=len(input_array)\n",
    "    for index, ele in zip(range(length),input_array):\n",
    "        # print(ele,index)\n",
    "        p_map[str(ele)] = index\n",
    "    return p_map\n",
    "\n",
    "# decoder to convert nodes back to entities\n",
    "def decode_map(encode_map):\n",
    "    de_map={}\n",
    "    for k,v in encode_map.items():\n",
    "        # index,ele \n",
    "        de_map[v]=k\n",
    "    return de_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b2304056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that return the encode mapping dictionary and decode mapping dictionary of an input graph\n",
    "# Append the global datafram variable graph_features_pdf with the encoded mapping\n",
    "def node_mapping(input_df):\n",
    "    encode_map_dictionary = {}\n",
    "    decode_map_dictionary = {}\n",
    "    map_count = {}\n",
    "    \n",
    "    for (columnName, columnData) in input_df.iteritems():\n",
    "        # print('Column Name : ', columnName)\n",
    "        # print('Column Contents : ', len(columnData.values))\n",
    "        encode_map_dictionary[columnName] = encode_map(set(input_df[columnName].values))\n",
    "        decode_map_dictionary[columnName] = decode_map(encode_map_dictionary[columnName])\n",
    "        \n",
    "    for (columnName, columnData) in input_df.iteritems():\n",
    "        graph_features_pdf[columnName + '_id_encoded'] = graph_features_pdf[columnName].apply(lambda e: encode_map_dictionary[columnName].get(str(e),-1))\n",
    "        \n",
    "        map_count[columnName + '_id_encoded'] =len(set(graph_features_pdf[columnName + '_id_encoded'].values))\n",
    "        \n",
    "    return encode_map_dictionary, decode_map_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f302d63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = './source_data_test.csv'  # file location\n",
    "\n",
    "data = pd.read_csv(file,encoding='utf-8',sep=None,on_bad_lines='skip',engine='python')\n",
    "columns = data.columns.tolist()   # load content into list\n",
    "# print(columns)\n",
    "data.head()\n",
    "# print(data.head())\n",
    "\n",
    "# Using DataFrame.copy() to create new dataframe.\n",
    "test_graph = data.copy()\n",
    "# print(test_graph)\n",
    "\n",
    "# Initial a global datafram variable to store a copy of the original graph.\n",
    "graph_features_pdf = test_graph\n",
    "\n",
    "# Construct an encoded graph for tensor calculation and store the encode/decode map \n",
    "encode_dic, decode_dic = node_mapping(test_graph)\n",
    "# graph_features_pdf.to_csv('graph_features_pdf.csv',index=False)\n",
    "# print(graph_features_pdf)\n",
    "index = int(len(graph_features_pdf.columns)/2)\n",
    "final_graph_pdf = graph_features_pdf[graph_features_pdf.columns[index:len(graph_features_pdf.columns)]].sort_values(by=graph_features_pdf.columns[index], ascending=True)\n",
    "# print(final_graph_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8db239dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes={'catalogNumber_id_encoded': 52091, 'country_id_encoded': 10, 'recordedBy_id_encoded': 2702},\n",
      "      num_edges={('catalogNumber_id_encoded', 'located_in', 'country_id_encoded'): 86782, ('catalogNumber_id_encoded', 'recorded_by', 'recordedBy_id_encoded'): 86782, ('country_id_encoded', 'bi_located_in', 'catalogNumber_id_encoded'): 86782, ('country_id_encoded', 'bi_travelled_to', 'recordedBy_id_encoded'): 86782, ('recordedBy_id_encoded', 'bi_recorded_by', 'catalogNumber_id_encoded'): 86782, ('recordedBy_id_encoded', 'travelled_to', 'country_id_encoded'): 86782},\n",
      "      metagraph=[('catalogNumber_id_encoded', 'country_id_encoded', 'located_in'), ('catalogNumber_id_encoded', 'recordedBy_id_encoded', 'recorded_by'), ('country_id_encoded', 'catalogNumber_id_encoded', 'bi_located_in'), ('country_id_encoded', 'recordedBy_id_encoded', 'bi_travelled_to'), ('recordedBy_id_encoded', 'catalogNumber_id_encoded', 'bi_recorded_by'), ('recordedBy_id_encoded', 'country_id_encoded', 'travelled_to')])\n"
     ]
    }
   ],
   "source": [
    "# src_col_list = final_graph_pdf.columns.tolist()\n",
    "src_col_list = ['catalogNumber_id_encoded', 'catalogNumber_id_encoded', 'recordedBy_id_encoded']\n",
    "dst_col_list = ['country_id_encoded', 'recordedBy_id_encoded', 'country_id_encoded']\n",
    "\n",
    "# specimen located in country\n",
    "located_in_src = final_graph_pdf[src_col_list[0]].values\n",
    "located_in_dst = final_graph_pdf[dst_col_list[0]].values\n",
    "# specimen recorded_by collector\n",
    "recorded_by_src = final_graph_pdf[src_col_list[1]].values\n",
    "recorded_by_dst = final_graph_pdf[dst_col_list[1]].values\n",
    "# collector travelled to country \n",
    "travelled_to_src = final_graph_pdf[src_col_list[2]].values\n",
    "travelled_to_dst = final_graph_pdf[dst_col_list[2]].values\n",
    "\n",
    "# label is not necessary in unsupervised learning  \n",
    "# catalog_node_buy_label = final_graph_pdf['label'].values\n",
    "\n",
    "# Construct the hetreograph with specified nodes and relations. \n",
    "# Nodes and relations could imported from separate csv using pd.read_csv\n",
    "# The botanical knowledge graph should be bidirectional\n",
    "# The demo here assume all nodes connected with valid info\n",
    "hetero_graph = dgl.heterograph({\n",
    "    (src_col_list[0], 'located_in', dst_col_list[0]): (located_in_src, located_in_dst),\n",
    "    (dst_col_list[0], 'bi_located_in', src_col_list[0]): (located_in_dst, located_in_src),\n",
    "    (src_col_list[1], 'recorded_by', dst_col_list[1]): (recorded_by_src, recorded_by_dst),\n",
    "    (dst_col_list[1], 'bi_recorded_by', src_col_list[1]): (recorded_by_dst, recorded_by_src),\n",
    "    (src_col_list[2], 'travelled_to', dst_col_list[2]): (travelled_to_src, travelled_to_dst),\n",
    "    (dst_col_list[2], 'bi_travelled_to', src_col_list[2]): (travelled_to_dst, travelled_to_src)\n",
    "})\n",
    "\n",
    "print(hetero_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ed558e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "located_in_count 86782\n",
      "recorded_by_count 86782\n"
     ]
    }
   ],
   "source": [
    "# specimen located in country\n",
    "located_in_count = len(located_in_dst)\n",
    "print(\"located_in_count\", located_in_count)\n",
    "\n",
    "# specimen recorded_by collector\n",
    "recorded_by_count = len(recorded_by_dst)\n",
    "print(\"recorded_by_count\", recorded_by_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424f16d",
   "metadata": {},
   "source": [
    "### Heterogeneous Implementation of Relational Graph Convolution Network\n",
    "\n",
    "    This section will show the model logic with a simple demo code.\n",
    "\n",
    "    In heterogeneous graphs for link prediction, we use the embedding information of the nodes on both sides of the edge and judge the existence or non-existence of the edge based on factors such as similarity and relevance. It is worth noting that although the information of the two nodes on both sides of the edge is used, from the perspective of multiple rounds of training, these two nodes also integrate the local structure and global properties of the surrounding nodes and make judgments based on the spatial structure of the graph. We need to sample negative edges for link prediction. Positive samples are constructed based on the existing node relationships, while negative samples are composed of randomly sampled edges in the graph. The model training is based on the loss constructed from the homophily assumption that nodes with closer proximity have similar characteristics. This can be considered as the essence of link (relationship) prediction in graphs.\n",
    "\n",
    "    GCN series models are spatial structures that are non-Euclidean structures. Every node has its embedding in GNN or GCN series models. Each copies its information along the edge through a message-passing process into the neighbour node's mailbox. Then, each node can obtain the embeddings sent by its neighbours from its mailbox and somehow aggregate its embedding and the neighbour node's embeddings. Standard aggregation methods include Mean and Max operations, among others. Furthermore, since the information from multiple neighbours forms a sequence, we can use RNN, LSTM, or similar approaches for aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0c5663",
   "metadata": {},
   "source": [
    "Heterogeneous graph convolutional layer model calls HeteroGraphConv function to convolute various relationships and then fuse nodes of the same type. In the graph convolution, there are model parameters such as weights. If these weights are not passed from the outside, creating them within the convolution essentially adds a fully connected layer to the model. This allows the convolution to be computed for each type of edge within the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "303afa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelGraphConvLayer(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_feat,\n",
    "                 out_feat,\n",
    "                 rel_names,\n",
    "                 num_bases,\n",
    "                 *,\n",
    "                 weight=True,\n",
    "                 bias=True,\n",
    "                 activation=None,\n",
    "                 regularizer=\"bdd\",\n",
    "                 self_loop=False,\n",
    "                 dropout=0.2):\n",
    "        \n",
    "        super(RelGraphConvLayer, self).__init__()\n",
    "        self.in_feat = in_feat\n",
    "        self.out_feat = out_feat\n",
    "        self.rel_names = rel_names\n",
    "        self.num_bases = num_bases\n",
    "        self.bias = bias\n",
    "        self.activation = activation\n",
    "        self.regularizer = regularizer\n",
    "        self.self_loop = self_loop\n",
    "        \n",
    "        # Only for computation purposes and does not store data. \n",
    "        self.conv = HeteroGraphConv({\n",
    "            rel: GraphConv(in_feat, out_feat, norm='right', weight=False, bias=False)\n",
    "            for rel in rel_names\n",
    "        })\n",
    "\n",
    "        self.use_weight = weight\n",
    "        self.use_basis = num_bases < len(self.rel_names) and weight\n",
    "        if self.use_weight:\n",
    "            if self.use_basis:\n",
    "                self.basis = dglnn.WeightBasis((in_feat, out_feat), num_bases, len(self.rel_names))\n",
    "            else:\n",
    "                # Each relation, set another weight, fully connected layer\n",
    "                self.weight = nn.Parameter(th.Tensor(len(self.rel_names), in_feat, out_feat))\n",
    "                nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "        # bias\n",
    "        if bias:\n",
    "            self.h_bias = nn.Parameter(th.Tensor(out_feat))\n",
    "            nn.init.zeros_(self.h_bias)\n",
    "\n",
    "        # weight for self loop\n",
    "        if self.self_loop:\n",
    "            self.loop_weight = nn.Parameter(th.Tensor(in_feat, out_feat))\n",
    "            nn.init.xavier_uniform_(self.loop_weight,\n",
    "                                    gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Each relation has a weight matrix corresponding to the input dimension and output dimension\n",
    "        g = g.local_var()\n",
    "        if self.use_weight:\n",
    "            weight = self.basis() if self.use_basis else self.weight\n",
    "            wdict = {self.rel_names[i]: {'weight': w.squeeze(0)}\n",
    "                     for i, w in enumerate(th.split(weight, 1, dim=0))}\n",
    "        else:\n",
    "            wdict = {}\n",
    "\n",
    "        if g.is_block:\n",
    "            inputs_src = inputs\n",
    "            inputs_dst = {k: v[:g.number_of_dst_nodes(k)] for k, v in inputs.items()}\n",
    "        else:\n",
    "            inputs_src = inputs_dst = inputs\n",
    "\n",
    "        # Output of the multi-type edge node convolution \n",
    "        # Input: blocks, embeding\n",
    "        hs = self.conv(g, inputs, mod_kwargs=wdict)\n",
    "\n",
    "        def _apply(ntype, h):\n",
    "            if self.self_loop:\n",
    "                h = h + th.matmul(inputs_dst[ntype], self.loop_weight)\n",
    "            if self.bias:\n",
    "                h = h + self.h_bias\n",
    "            if self.activation:\n",
    "                h = self.activation(h)\n",
    "            return self.dropout(h)\n",
    "\n",
    "        #\n",
    "        return {ntype: _apply(ntype, h) for ntype, h in hs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "277f8c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This python class only returns a dictionary, but this dictionary includes multiple Embeding Variables. \n",
    "Note that the Variables here can all be updated as the network training changes. \n",
    "We can get the Embeding of the corresponding element according to the node type and node ID\n",
    "\"\"\"\n",
    "class RelGraphEmbed(nn.Module):\n",
    "    r\"\"\"Embedding layer for featureless heterograph.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 g,\n",
    "                 embed_size,\n",
    "                 embed_name='embed',\n",
    "                 activation=None,\n",
    "                 dropout=0.0):\n",
    "        super(RelGraphEmbed, self).__init__()\n",
    "        self.g = g\n",
    "        self.embed_size = embed_size\n",
    "        self.embed_name = embed_name\n",
    "        self.activation = activation\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # create weight embeddings for each node for each relation\n",
    "        self.embeds = nn.ParameterDict()\n",
    "        for ntype in g.ntypes:\n",
    "            embed = nn.Parameter(torch.Tensor(g.number_of_nodes(ntype), self.embed_size))\n",
    "            nn.init.xavier_uniform_(embed, gain=nn.init.calculate_gain('relu'))\n",
    "            self.embeds[ntype] = embed\n",
    "\n",
    "    def forward(self, block=None):\n",
    "        \n",
    "        return self.embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8a986655",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The RGCN structure that includes the forward method for training and the inference method\n",
    "The inference method can be used to export the embedding of each node\n",
    "\"\"\"\n",
    "class EntityClassify(nn.Module):\n",
    "    def __init__(self,\n",
    "                 g,\n",
    "                 h_dim, out_dim,\n",
    "                 num_bases=-1,\n",
    "                 num_hidden_layers=1,\n",
    "                 dropout=0,\n",
    "                 use_self_loop=False):\n",
    "        super(EntityClassify, self).__init__()\n",
    "        self.g = g\n",
    "        self.h_dim = h_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.rel_names = list(set(g.etypes))\n",
    "        self.rel_names.sort()\n",
    "        if num_bases < 0 or num_bases > len(self.rel_names):\n",
    "            self.num_bases = len(self.rel_names)\n",
    "        else:\n",
    "            self.num_bases = num_bases\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.dropout = dropout\n",
    "        self.use_self_loop = use_self_loop\n",
    "\n",
    "        self.embed_layer = RelGraphEmbed(g, self.h_dim)\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # i2h\n",
    "        self.layers.append(RelGraphConvLayer(\n",
    "            self.h_dim, self.h_dim, self.rel_names,\n",
    "            self.num_bases, activation=F.relu, self_loop=self.use_self_loop,\n",
    "            dropout=self.dropout, weight=False))\n",
    "\n",
    "        # h2h , No hidden layer is added here, only 2 layers of convolution are used\n",
    "        # Remove comment-out for adding hidden layer\n",
    "        # for i in range(self.num_hidden_layers):\n",
    "        #    self.layers.append(RelGraphConvLayer(\n",
    "        #        self.h_dim, self.h_dim, self.rel_names,\n",
    "        #        self.num_bases, activation=F.relu, self_loop=self.use_self_loop,\n",
    "        #        dropout=self.dropout))\n",
    "        \n",
    "        # h2o\n",
    "        self.layers.append(RelGraphConvLayer(\n",
    "            self.h_dim, self.out_dim, self.rel_names,\n",
    "            self.num_bases, activation=None,\n",
    "            self_loop=self.use_self_loop))\n",
    "\n",
    "    # Input: blocks, embeding\n",
    "    def forward(self, h=None, blocks=None):\n",
    "        if h is None:\n",
    "            # full graph training\n",
    "            h = self.embed_layer()\n",
    "        if blocks is None:\n",
    "            # full graph training\n",
    "            for layer in self.layers:\n",
    "                h = layer(self.g, h)\n",
    "        else:\n",
    "            # minibatch training\n",
    "            # Input: blocks, embeding\n",
    "            for layer, block in zip(self.layers, blocks):\n",
    "                h = layer(block, h)\n",
    "        return h\n",
    "\n",
    "    \n",
    "    def inference(self, g, batch_size, device=\"cpu\", num_workers=0, x=None):\n",
    "\n",
    "        if x is None:\n",
    "            x = self.embed_layer()\n",
    "\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            y = {\n",
    "                k: th.zeros(\n",
    "                    g.number_of_nodes(k),\n",
    "                    self.h_dim if l != len(self.layers) - 1 else self.out_dim)\n",
    "                for k in g.ntypes}\n",
    "\n",
    "            \n",
    "            sampler = dgl.dataloading.MultiLayerFullNeighborSampler(1)\n",
    "            dataloader = dgl.dataloading.NodeDataLoader(\n",
    "                g,\n",
    "                {k: th.arange(g.number_of_nodes(k)) for k in g.ntypes},\n",
    "                sampler,\n",
    "                batch_size = batch_size,\n",
    "                shuffle = True,\n",
    "                drop_last = False,\n",
    "                num_workers = num_workers)\n",
    "            \n",
    "            for input_nodes, output_nodes, blocks in tqdm.tqdm(dataloader):\n",
    "                # print(input_nodes)\n",
    "                block = blocks[0].to(device)\n",
    "                        \n",
    "                h = {k: x[k][input_nodes[k]].to(device) for k in input_nodes.keys()}\n",
    "                h = layer(block, h)\n",
    "\n",
    "                for k in h.keys():\n",
    "                    y[k][output_nodes[k]] = h[k].cpu()\n",
    "\n",
    "            x = y\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e03a2c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embedding according to node type and node ID for training update\n",
    "def extract_embed(node_embed, input_nodes):\n",
    "    emb = {}\n",
    "    for ntype, nid in input_nodes.items():\n",
    "        nid = input_nodes[ntype]\n",
    "        emb[ntype] = node_embed[ntype][nid]\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63991803",
   "metadata": {},
   "source": [
    "    Obtaining information from neighbours to update the model might make nodes closer to each other more similar, which has both advantages and disadvantages.\n",
    "    - The advantage is that it aligns with the fundamental goal of training graph models: to learn the relationships between nodes in the graph. The semantic information of these relationships is contained within the embeddings.\n",
    "    - However, the disadvantage is that collecting too many neighbours during training or having too many epochs of full-graph update training may lead to weak differentiation between nodes, similar embeddings, and the over-smoothing problem.\n",
    "\n",
    "    In more detail, training a link prediction model involves comparing the score between a seed node and a connected node and the difference between the seed node's score and the score of any other node. For example, given an edge connecting 𝑢 and 𝑣, a good model would expect the score between 𝑢 and 𝑣 to be higher than the score between 𝑢 and a node 𝑣′ sampled from an arbitrary noise distribution 𝑃𝑛(𝑣). The method of selecting neighbour nodes from an arbitrary noise distribution is called negative sampling in graph data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7926c34a",
   "metadata": {},
   "source": [
    "Instead of using full-graph training, we import the EdgeDataLoader function to compute subgraphs with batch training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c7477b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbax9qg2/anaconda3/envs/dgl/lib/python3.9/site-packages/dgl/dataloading/dataloader.py:968: DGLWarning: EdgeDataLoader directly taking a BlockSampler will be deprecated and it will not support feature prefetching. Please use dgl.dataloading.as_edge_prediction_sampler to wrap it.\n",
      "  dgl_warning(\n"
     ]
    }
   ],
   "source": [
    "neg_sample_count = 1\n",
    "batch_size=500 # set batch size\n",
    "\n",
    "sampler = MultiLayerFullNeighborSampler(2)  # sampling all nodes in second layer\n",
    "\n",
    "\"\"\"\n",
    "Batch training -- Select the positive graph composed of the edges of the seed nodes,\n",
    "and the negative graph consisting of the seed nodes and their randomly sampled global nodes \n",
    "\"\"\"\n",
    "hetero_graph.edges['located_in'].data['train_mask'] = torch.zeros(located_in_count, dtype=torch.bool).bernoulli(1.0)\n",
    "train_ip_eids = hetero_graph.edges['located_in'].data['train_mask'].nonzero(as_tuple=True)[0]\n",
    "ip_dataloader = EdgeDataLoader(\n",
    "    hetero_graph, {'located_in': train_ip_eids}, sampler, negative_sampler=Uniform(neg_sample_count), batch_size=batch_size\n",
    ")\n",
    "hetero_graph.edges['recorded_by'].data['train_mask'] = torch.zeros(recorded_by_count, dtype=torch.bool).bernoulli(1.0)\n",
    "train_item_eids = hetero_graph.edges['recorded_by'].data['train_mask'].nonzero(as_tuple=True)[0]\n",
    "item_dataloader = EdgeDataLoader(\n",
    "    hetero_graph, {'recorded_by': train_item_eids}, sampler, negative_sampler=Uniform(neg_sample_count), batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "cd541cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Heterograph Conv model\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, graph, hidden_feat_dim, out_feat_dim):\n",
    "        super().__init__()\n",
    "        self.rgcn = EntityClassify(graph,\n",
    "                                   hidden_feat_dim,\n",
    "                                   out_feat_dim)\n",
    "        self.pred = HeteroDotProductPredictor()\n",
    "\n",
    "    def forward(self, h, pos_g, neg_g, blocks, etype):\n",
    "        h = self.rgcn(h, blocks)\n",
    "        return self.pred(pos_g, h, etype), self.pred(neg_g, h, etype)\n",
    "\n",
    "\n",
    "class MarginLoss(nn.Module):\n",
    "\n",
    "    def forward(self, pos_score, neg_score):\n",
    "        # Compute average of loss, changes the shape of tensor\n",
    "        # 1- pos_score + neg_score\n",
    "        return (1 - pos_score + neg_score.view(pos_score.shape[0], -1)).clamp(min=0).mean()\n",
    "\n",
    "\n",
    "class HeteroDotProductPredictor(nn.Module):\n",
    "\n",
    "    def forward(self, graph, h, etype):\n",
    "        # update h and saved it for global\n",
    "        # h contains the node representations for each edge type computed from node_clf_hetero.py\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h  # assigns 'h' of all node types in one shot\n",
    "            graph.apply_edges(fn.u_dot_v('h', 'h', 'score'), etype=etype)\n",
    "            return graph.edges[etype].data['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f9b4208d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_feats = hetero_graph.nodes['type'].data['feature'].shape[1]\n",
    "n_hetero_features = 16 # random set for demo\n",
    "hidden_feat_dim = n_hetero_features\n",
    "out_feat_dim = n_hetero_features\n",
    "\n",
    "embed_layer = RelGraphEmbed(hetero_graph, hidden_feat_dim)\n",
    "all_node_embed = embed_layer()\n",
    "\n",
    "model = Model(hetero_graph, hidden_feat_dim, out_feat_dim)\n",
    "# Parameter optimisation: weight and input embedding\n",
    "all_params = itertools.chain(model.parameters(), embed_layer.parameters())\n",
    "optimizer = torch.optim.Adam(all_params, lr=0.01, weight_decay=0)\n",
    "\n",
    "loss_func = MarginLoss() # can change to self-defined loss function if needed\n",
    "\n",
    "def train_etype_one_epoch(etype, spec_dataloader):\n",
    "    losses = []\n",
    "    # input_nodes is a collection of all nodes in the sampled subgraph\n",
    "    for input_nodes, pos_g, neg_g, blocks in tqdm.tqdm(spec_dataloader):\n",
    "        emb = extract_embed(all_node_embed, input_nodes)\n",
    "        pos_score, neg_score = model(emb, pos_g, neg_g, blocks, etype)\n",
    "        loss = loss_func(pos_score, neg_score)\n",
    "        losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('{:s} Epoch {:d} | Loss {:.4f}'.format(etype, epoch, sum(losses) / len(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "dc487b3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 174/174 [00:36<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "located_in Epoch 0 | Loss 0.1557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 174/174 [00:31<00:00,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recorded_by Epoch 0 | Loss 0.1372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# An example to show how to run the training\n",
    "for epoch in range(1):\n",
    "    print(\"start epoch:\", epoch)\n",
    "    model.train()\n",
    "    train_etype_one_epoch('located_in', ip_dataloader)\n",
    "    train_etype_one_epoch('recorded_by', item_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4cbd88c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_embed: tensor([-0.0177, -0.1573, -0.0390,  0.3429, -0.1840,  0.0620, -0.0962, -0.2726,\n",
      "         0.1229, -0.2841,  0.1398, -0.0286,  0.0429, -0.1783, -0.1304, -0.0655],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "ParameterDict(\n",
      "    (catalogNumber_id_encoded): Parameter containing: [torch.FloatTensor of size 52091x16]\n",
      "    (country_id_encoded): Parameter containing: [torch.FloatTensor of size 10x16]\n",
      "    (recordedBy_id_encoded): Parameter containing: [torch.FloatTensor of size 2702x16]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Save graph and data to bin\n",
    "save_graphs(\"graph.bin\", [hetero_graph])\n",
    "torch.save(model.state_dict(), \"model.bin\")\n",
    "\n",
    "# Node embeding output by each node\n",
    "col = src_col_list[0]\n",
    "print(\"node_embed:\", all_node_embed[col][0])\n",
    "print(all_node_embed)\n",
    "\n",
    "# Return logit from inference\n",
    "# Set 0 to unlock\n",
    "# inference_out = model.inference(hetero_graph, batch_size, 'cpu', num_workers=0, x = all_node_embed)\n",
    "# print(inference_out[src_col_list[0]].shape)\n",
    "# print(inference_out[src_col_list[0]][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708d5fa5",
   "metadata": {},
   "source": [
    "### Methodology\n",
    "\n",
    "    Relational Graph Convolutional Networks (R-GCNs) are an extension of Graph Convolutional Networks (GCNs) designed to handle graphs with multiple edge types or relations, such as knowledge graphs. R-GCNs are helpful for tasks like node classification and link prediction in multi-relational graphs.\n",
    "    \n",
    "    Here's an overview of the methodology of R-GCNs with math equations:\n",
    "\n",
    "#### Message-passing framework:\n",
    "\n",
    "    R-GCNs are built on the message-passing framework, where each node aggregates messages (features) from its neighbours. In R-GCNs, the aggregation process also considers the edge types (relations) between nodes. Let G = (V, E) be a graph with nodes V and edges E. Each edge e_ij in E is associated with a relation type r ∈ R, where R is the set of all relation types in the graph. The feature matrix of nodes is represented by H^(0), where H^(0)_i is the feature vector of node i.\n",
    "\n",
    "#### R-GCN layer:\n",
    "\n",
    "    In an R-GCN layer, each node updates its features by aggregating information from its neighbours, considering the relation types between nodes. The updated features H^(l+1)_i for node i in layer l+1 are computed as follows:\n",
    "    \n",
    "    H^(l+1)i = σ(∑(j, r)∈N(i) (W_r^(l) H^(l)_j))\n",
    "    \n",
    "    Here, N(i) represents the set of neighbour nodes of node i, considering the relation type r. W_r^(l) is the weight matrix for relation type r in layer l, and σ is a non-linear activation function, such as ReLU.\n",
    "\n",
    "#### Weight sharing:\n",
    "\n",
    "    R-GCNs often use weight sharing or regularisation techniques, such as block-diagonal decomposition and basis decomposition, to handle many relation types and reduce the number of parameters in the model. \n",
    "    \n",
    "    Block-diagonal decomposition refers to a way of expressing a matrix as a block-diagonal form, where each block is a square matrix that corresponds to a specific subspace of the matrix.  In other words, the matrix is decomposed into a sum of square matrices that are each confined to a certain subset of rows and columns.  This decomposition can be useful for solving systems of equations or diagonalising matrices that are not necessarily diagonal or triangular.\n",
    "    \n",
    "    Basis decomposition, on the other hand, refers to a way of expressing a matrix as a linear combination of basis matrices, where the basis matrices are chosen to capture the important features or patterns in the matrix.  In other words, the matrix is decomposed into a sum of basis matrices that can be used to reconstruct the original matrix with minimal error.\n",
    "    Basis decomposition represents each relation-specific weight matrix W_r as a linear combination of basis weight matrices B_k:\n",
    "    \n",
    "    W_r = ∑_k α_rk B_k\n",
    "    \n",
    "    Here, α_rk is a scalar weight for basis matrix B_k and relation type r. This decomposition reduces the number of parameters in the model, mitigating overfitting and improving generalisation.\n",
    "    \n",
    "\n",
    "#### Final model:\n",
    "    An R-GCN model typically comprises multiple R-GCN layers, followed by a task-specific output layer. For node classification, a softmax output layer can be used, while for link prediction, a scoring function, such as DistMult or ComplEx, can be applied to compute the likelihood of potential links.\n",
    "\n",
    "    For training, a loss function specific to the task (e.g., cross-entropy loss for node classification or ranking loss for link prediction) is used. The model parameters are optimised using gradient-based optimisation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442c7feb",
   "metadata": {},
   "source": [
    "### Homogeneous Implementation of Relational Graph Convolution Network\n",
    "\n",
    "This implementation is an extended version based on the DGL built-in RGCN model and the official implementation (https://github.com/tkipf/relational-gcn). Details can be checked in the code comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b844688a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=54803, num_edges=520692,\n",
      "      ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), '_TYPE': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), '_TYPE': Scheme(shape=(), dtype=torch.int64)})\n",
      "True\n",
      "{'_ID': tensor([    0,     1,     2,  ..., 86779, 86780, 86781]), '_TYPE': tensor([0, 0, 0,  ..., 5, 5, 5]), 'train_mask': tensor([ True,  True,  True,  ..., False, False, False]), 'val_mask': tensor([False, False, False,  ..., False, False, False]), 'test_mask': tensor([False, False, False,  ...,  True,  True,  True]), 'etype': tensor([0, 0, 0,  ..., 5, 5, 5])}\n"
     ]
    }
   ],
   "source": [
    "# Converting the hetero graph into homogeneous \n",
    "# https://docs.dgl.ai/en/0.9.x/generated/dgl.to_homogeneous.html\n",
    "test_homograph = dgl.to_homogeneous(hetero_graph)\n",
    "print(test_homograph)\n",
    "print(test_homograph.is_homogeneous)\n",
    "\n",
    "# Assign masks indicating whether a edge belongs to training, validation, and test set.\n",
    "e_nodes = a.num_edges()\n",
    "e_train = int(e_nodes * 0.6)\n",
    "e_val = int(e_nodes * 0.2)\n",
    "train_mask = torch.zeros(e_nodes, dtype=torch.bool)\n",
    "val_mask = torch.zeros(e_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(e_nodes, dtype=torch.bool)\n",
    "train_mask[:e_train] = True\n",
    "val_mask[e_train:e_train + e_val] = True\n",
    "test_mask[e_train + e_val:] = True\n",
    "test_homograph.edata['train_mask'] = train_mask\n",
    "test_homograph.edata['val_mask'] = val_mask\n",
    "test_homograph.edata['test_mask'] = test_mask\n",
    "\n",
    "test_homograph.edata['etype'] = test_homograph.edata['_TYPE']\n",
    "\n",
    "print(test_homograph.edata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e9ffe24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for building training and testing graphs\n",
    "# A helper method that allows the knowledge graph to be devided in the subgraphs for training\n",
    "def get_subset_g(g, mask, num_rels, bidirected=False):\n",
    "    src, dst = g.edges()\n",
    "    sub_src = src[mask]\n",
    "    sub_dst = dst[mask]\n",
    "    sub_rel = g.edata['etype'][mask]\n",
    "\n",
    "    if bidirected:\n",
    "        sub_src, sub_dst = th.cat([sub_src, sub_dst]), th.cat([sub_dst, sub_src])\n",
    "        sub_rel = th.cat([sub_rel, sub_rel + num_rels])\n",
    "\n",
    "    #  sub_g = dgl.graph((sub_src, sub_dst), num_nodes=g.num_nodes(), idtype=th.int32)\n",
    "    sub_g = dgl.graph((sub_src, sub_dst), num_nodes=g.num_nodes())\n",
    "    sub_g.edata[dgl.ETYPE] = sub_rel\n",
    "\n",
    "    return sub_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2157ad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method to initialise the sets of positive samples and negative samples\n",
    "class NegativeSampler:\n",
    "    def __init__(self, k=10):\n",
    "        self.k = k\n",
    "\n",
    "    def sample(self, pos_samples, num_nodes):\n",
    "        batch_size = len(pos_samples)\n",
    "        neg_batch_size = batch_size * self.k\n",
    "        neg_samples = np.tile(pos_samples, (self.k, 1))\n",
    "\n",
    "        values = np.random.randint(num_nodes, size=neg_batch_size)\n",
    "        choices = np.random.uniform(size=neg_batch_size)\n",
    "        subj = choices > 0.5\n",
    "        obj = choices <= 0.5\n",
    "        neg_samples[subj, 0] = values[subj]\n",
    "        neg_samples[obj, 2] = values[obj]\n",
    "        samples = np.concatenate((pos_samples, neg_samples))\n",
    "\n",
    "        # binary labels indicating positive and negative samples\n",
    "        labels = np.zeros(batch_size * (self.k + 1), dtype=np.float32)\n",
    "        labels[:batch_size] = 1\n",
    "\n",
    "        return th.from_numpy(samples), th.from_numpy(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2aa45922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating information from node's neighbours during message-passing process.\n",
    "# Global uniform for aggregating positive samples \n",
    "class GlobalUniform:\n",
    "    def __init__(self, g, sample_size):\n",
    "        self.sample_size = sample_size\n",
    "        self.eids = np.arange(g.num_edges())\n",
    "\n",
    "    def sample(self):\n",
    "        return th.from_numpy(np.random.choice(self.eids, self.sample_size))\n",
    "\n",
    "# Sample positive connected components by neighborhood expansions.\n",
    "# Introduced in the original publication\n",
    "class NeighborExpand:\n",
    "    \"\"\"Sample a connected component by neighborhood expansion\"\"\"\n",
    "    def __init__(self, g, sample_size):\n",
    "        self.g = g\n",
    "        self.nids = np.arange(g.num_nodes())\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "    def sample(self):\n",
    "        edges = th.zeros((self.sample_size), dtype=th.int32) \n",
    "        #  int64\n",
    "        neighbor_counts = (self.g.in_degrees() + self.g.out_degrees()).numpy()\n",
    "        seen_edge = np.array([False] * self.g.num_edges())\n",
    "        seen_node = np.array([False] * self.g.num_nodes())\n",
    "\n",
    "        for i in range(self.sample_size):\n",
    "            if np.sum(seen_node) == 0:\n",
    "                node_weights = np.ones_like(neighbor_counts)\n",
    "                node_weights[np.where(neighbor_counts == 0)] = 0\n",
    "            else:\n",
    "                # Sample a visited node if applicable.\n",
    "                # This guarantees a connected component.\n",
    "                node_weights = neighbor_counts * seen_node\n",
    "\n",
    "            node_probs = node_weights / np.sum(node_weights)\n",
    "            chosen_node = np.random.choice(self.nids, p=node_probs)\n",
    "\n",
    "            # Sample a neighbor of the sampled node\n",
    "            u1, v1, eid1 = self.g.in_edges(chosen_node, form='all')\n",
    "            u2, v2, eid2 = self.g.out_edges(chosen_node, form='all')\n",
    "            u = th.cat([u1, u2])\n",
    "            v = th.cat([v1, v2])\n",
    "            eid = th.cat([eid1, eid2])\n",
    "\n",
    "            to_pick = True\n",
    "            while to_pick:\n",
    "                random_id = th.randint(high=eid.shape[0], size=(1,))\n",
    "                chosen_eid = eid[random_id]\n",
    "                to_pick = seen_edge[chosen_eid]\n",
    "\n",
    "            chosen_u = u[random_id]\n",
    "            chosen_v = v[random_id]\n",
    "            edges[i] = chosen_eid\n",
    "            seen_node[chosen_u] = True\n",
    "            seen_node[chosen_v] = True\n",
    "            seen_edge[chosen_eid] = True\n",
    "\n",
    "            neighbor_counts[chosen_u] -= 1\n",
    "            neighbor_counts[chosen_v] -= 1\n",
    "\n",
    "        return edges\n",
    "\n",
    "class SubgraphIterator:\n",
    "    def __init__(self, g, num_rels, pos_sampler, sample_size=30000, num_epochs=6000): # 6000 epochs\n",
    "        self.g = g\n",
    "        self.num_rels = num_rels\n",
    "        self.sample_size = sample_size\n",
    "        self.num_epochs = num_epochs\n",
    "        if pos_sampler == 'neighbor':\n",
    "            self.pos_sampler = NeighborExpand(g, sample_size)\n",
    "        else:\n",
    "            self.pos_sampler = GlobalUniform(g, sample_size)\n",
    "        self.neg_sampler = NegativeSampler()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_epochs\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        eids = self.pos_sampler.sample()\n",
    "        src, dst = self.g.find_edges(eids)\n",
    "        src, dst = src.numpy(), dst.numpy()\n",
    "        rel = self.g.edata[dgl.ETYPE][eids.long()].numpy()\n",
    "\n",
    "        # relabel nodes to have consecutive node IDs\n",
    "        uniq_v, edges = np.unique((src, dst), return_inverse=True)\n",
    "        num_nodes = len(uniq_v)\n",
    "        # edges is the concatenation of src and dst with relabeled ID\n",
    "        src, dst = np.reshape(edges, (2, -1))\n",
    "        relabeled_data = np.stack((src, rel, dst)).transpose()\n",
    "\n",
    "        samples, labels = self.neg_sampler.sample(relabeled_data, num_nodes)\n",
    "\n",
    "        # use only half of the positive edges\n",
    "        chosen_ids = np.random.choice(np.arange(self.sample_size),\n",
    "                                      size=int(self.sample_size / 2),\n",
    "                                      replace=False)\n",
    "        src = src[chosen_ids]\n",
    "        dst = dst[chosen_ids]\n",
    "        rel = rel[chosen_ids]\n",
    "        src, dst = np.concatenate((src, dst)), np.concatenate((dst, src))\n",
    "        rel = np.concatenate((rel, rel + self.num_rels))\n",
    "        sub_g = dgl.graph((src, dst), num_nodes=num_nodes, idtype=th.int32)\n",
    "        # sub_g = dgl.graph((src, dst), num_nodes=num_nodes)\n",
    "        sub_g.edata[dgl.ETYPE] = th.from_numpy(rel)\n",
    "        sub_g.edata['norm'] = dgl.norm_by_dst(sub_g).unsqueeze(-1)\n",
    "        uniq_v = th.from_numpy(uniq_v).view(-1).long()\n",
    "\n",
    "        return sub_g, uniq_v, samples, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "512a2ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for evaluations (raw)\n",
    "def perturb_and_get_raw_rank(emb, w, a, r, b, test_size, batch_size=100):\n",
    "    \"\"\" Perturb one element in the triplets\"\"\"\n",
    "    n_batch = (test_size + batch_size - 1) // batch_size\n",
    "    ranks = []\n",
    "    emb = emb.transpose(0, 1) # size D x V\n",
    "    w = w.transpose(0, 1)     # size D x R\n",
    "    for idx in range(n_batch):\n",
    "        print(\"batch {} / {}\".format(idx, n_batch))\n",
    "        batch_start = idx * batch_size\n",
    "        batch_end = (idx + 1) * batch_size\n",
    "        batch_a = a[batch_start: batch_end]\n",
    "        batch_r = r[batch_start: batch_end]\n",
    "        emb_ar = emb[:,batch_a] * w[:,batch_r] # size D x E\n",
    "        emb_ar = emb_ar.unsqueeze(2)           # size D x E x 1\n",
    "        emb_c = emb.unsqueeze(1)               # size D x 1 x V\n",
    "\n",
    "        # out-prod and reduce sum\n",
    "        out_prod = th.bmm(emb_ar, emb_c)          # size D x E x V\n",
    "        score = th.sum(out_prod, dim=0).sigmoid() # size E x V\n",
    "        target = b[batch_start: batch_end]\n",
    "\n",
    "        _, indices = th.sort(score, dim=1, descending=True)\n",
    "        indices = th.nonzero(indices == target.view(-1, 1), as_tuple=False)\n",
    "        ranks.append(indices[:, 1].view(-1))\n",
    "    return th.cat(ranks)\n",
    "\n",
    "# Utility functions for evaluations (filtered)\n",
    "def filter(triplets_to_filter, target_s, target_r, target_o, num_nodes, filter_o=True):\n",
    "    \"\"\"Get candidate heads or tails to score\"\"\"\n",
    "    target_s, target_r, target_o = int(target_s), int(target_r), int(target_o)\n",
    "\n",
    "    # Add the ground truth node first\n",
    "    if filter_o:\n",
    "        candidate_nodes = [target_o]\n",
    "    else:\n",
    "        candidate_nodes = [target_s]\n",
    "\n",
    "    for e in range(num_nodes):\n",
    "        triplet = (target_s, target_r, e) if filter_o else (e, target_r, target_o)\n",
    "        # Do not consider a node if it leads to a real triplet\n",
    "        if triplet not in triplets_to_filter:\n",
    "            candidate_nodes.append(e)\n",
    "    return th.LongTensor(candidate_nodes)\n",
    "\n",
    "def perturb_and_get_filtered_rank(emb, w, s, r, o, test_size, triplets_to_filter, filter_o=True):\n",
    "    \"\"\"Perturb subject or object in the triplets\"\"\"\n",
    "    num_nodes = emb.shape[0]\n",
    "    ranks = []\n",
    "    for idx in range(test_size):\n",
    "        if idx % 100 == 0:\n",
    "            print(\"test triplet {} / {}\".format(idx, test_size))\n",
    "        target_s = s[idx]\n",
    "        target_r = r[idx]\n",
    "        target_o = o[idx]\n",
    "        candidate_nodes = filter(triplets_to_filter, target_s, target_r,\n",
    "                                 target_o, num_nodes, filter_o=filter_o)\n",
    "        if filter_o:\n",
    "            emb_s = emb[target_s]\n",
    "            emb_o = emb[candidate_nodes]\n",
    "        else:\n",
    "            emb_s = emb[candidate_nodes]\n",
    "            emb_o = emb[target_o]\n",
    "        target_idx = 0\n",
    "        emb_r = w[target_r]\n",
    "        emb_triplet = emb_s * emb_r * emb_o\n",
    "        scores = th.sigmoid(th.sum(emb_triplet, dim=1))\n",
    "\n",
    "        _, indices = th.sort(scores, descending=True)\n",
    "        rank = int((indices == target_idx).nonzero())\n",
    "        ranks.append(rank)\n",
    "    return th.LongTensor(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7f0ae625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to compute the Mean Reciprocal Rank (MRR)\n",
    "def _calc_mrr(emb, w, test_mask, triplets_to_filter, batch_size, filter=False):\n",
    "    with th.no_grad():\n",
    "        test_triplets = triplets_to_filter[test_mask]\n",
    "        s, r, o = test_triplets[:,0], test_triplets[:,1], test_triplets[:,2]\n",
    "        test_size = len(s)\n",
    "\n",
    "        if filter:\n",
    "            metric_name = 'MRR (filtered)'\n",
    "            triplets_to_filter = {tuple(triplet) for triplet in triplets_to_filter.tolist()}\n",
    "            ranks_s = perturb_and_get_filtered_rank(emb, w, s, r, o, test_size,\n",
    "                                                    triplets_to_filter, filter_o=False)\n",
    "            ranks_o = perturb_and_get_filtered_rank(emb, w, s, r, o,\n",
    "                                                    test_size, triplets_to_filter)\n",
    "        else:\n",
    "            metric_name = 'MRR (raw)'\n",
    "            ranks_s = perturb_and_get_raw_rank(emb, w, o, r, s, test_size, batch_size)\n",
    "            ranks_o = perturb_and_get_raw_rank(emb, w, s, r, o, test_size, batch_size)\n",
    "\n",
    "        ranks = th.cat([ranks_s, ranks_o])\n",
    "        ranks += 1 # change to 1-indexed\n",
    "        mrr = th.mean(1.0 / ranks.float()).item()\n",
    "        print(\"{}: {:.6f}\".format(metric_name, mrr))\n",
    "\n",
    "    return mrr\n",
    "\n",
    "# Main evaluation function\n",
    "def calc_mrr(emb, w, test_mask, triplets, batch_size=100, eval_p=\"filtered\"):\n",
    "    if eval_p == \"filtered\":\n",
    "        mrr = _calc_mrr(emb, w, test_mask, triplets, batch_size, filter=True)\n",
    "    else:\n",
    "        mrr = _calc_mrr(emb, w, test_mask, triplets, batch_size)\n",
    "    return mrr\n",
    "\n",
    "class RGCN(nn.Module):\n",
    "    def __init__(self, num_nodes, h_dim, out_dim, num_rels,\n",
    "                 regularizer=\"basis\", num_bases=-1, dropout=0.,\n",
    "                 self_loop=False,\n",
    "                 ns_mode=False):\n",
    "        super(RGCN, self).__init__()\n",
    "\n",
    "        if num_bases == -1:\n",
    "            num_bases = num_rels\n",
    "        self.emb = nn.Embedding(num_nodes, h_dim)\n",
    "        self.conv1 = RelGraphConv(h_dim, h_dim, num_rels, regularizer,\n",
    "                                  num_bases, self_loop=self_loop)\n",
    "        self.conv2 = RelGraphConv(h_dim, out_dim, num_rels, regularizer, num_bases, self_loop=self_loop)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ns_mode = ns_mode\n",
    "\n",
    "    def forward(self, g, nids=None):\n",
    "        if self.ns_mode:\n",
    "            # forward for neighbor sampling\n",
    "            x = self.emb(g[0].srcdata[dgl.NID])\n",
    "            h = self.conv1(g[0], x, g[0].edata[dgl.ETYPE], g[0].edata['norm'])\n",
    "            h = self.dropout(F.relu(h))\n",
    "            h = self.conv2(g[1], h, g[1].edata[dgl.ETYPE], g[1].edata['norm'])\n",
    "            return h\n",
    "        else:\n",
    "            x = self.emb.weight if nids is None else self.emb(nids)\n",
    "            h = self.conv1(g, x, g.edata[dgl.ETYPE], g.edata['norm'])\n",
    "            h = self.dropout(F.relu(h))\n",
    "            h = self.conv2(g, h, g.edata[dgl.ETYPE], g.edata['norm'])\n",
    "            return h\n",
    "\n",
    "\n",
    "class LinkPredict(nn.Module):\n",
    "    def __init__(self, in_dim, num_rels, h_dim=500, num_bases=100, dropout=0.2, reg_param=0.01):\n",
    "        super(LinkPredict, self).__init__()\n",
    "        \n",
    "        # Feel free to test with different regularizers \n",
    "        # self.rgcn = RGCN(in_dim, h_dim, h_dim, num_rels * 2, regularizer=\"basis\",\n",
    "        #                  num_bases=num_bases, dropout=dropout, self_loop=True)\n",
    "        \n",
    "        self.rgcn = RGCN(in_dim, h_dim, h_dim, num_rels * 2, regularizer=\"bdd\",\n",
    "                          num_bases=num_bases, dropout=dropout, self_loop=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.reg_param = reg_param\n",
    "        self.w_relation = nn.Parameter(th.Tensor(num_rels, h_dim))\n",
    "        nn.init.xavier_uniform_(self.w_relation,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def calc_score(self, embedding, triplets):\n",
    "        # DistMult\n",
    "        s = embedding[triplets[:,0]]\n",
    "        r = self.w_relation[triplets[:,1]]\n",
    "        o = embedding[triplets[:,2]]\n",
    "        score = th.sum(s * r * o, dim=1)\n",
    "        return score\n",
    "\n",
    "    def forward(self, g, nids):\n",
    "        return self.dropout(self.rgcn(g, nids=nids))\n",
    "\n",
    "    def regularization_loss(self, embedding):\n",
    "        return th.mean(embedding.pow(2)) + th.mean(self.w_relation.pow(2))\n",
    "\n",
    "    def get_loss(self, embed, triplets, labels):\n",
    "        # each row in the triplets is a 3-tuple of (source, relation, destination)\n",
    "        score = self.calc_score(embed, triplets)\n",
    "        predict_loss = F.binary_cross_entropy_with_logits(score, labels)\n",
    "        reg_loss = self.regularization_loss(embed)\n",
    "        return predict_loss + self.reg_param * reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "359d301d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with DGL built-in RGCN module\n",
      "Epoch 0000 | Loss 3.2731 | Best MRR 0.0000\n",
      "Epoch 0001 | Loss 14.6303 | Best MRR 0.0000\n",
      "Epoch 0002 | Loss 56.9931 | Best MRR 0.0000\n",
      "Epoch 0003 | Loss 11.4010 | Best MRR 0.0000\n",
      "Epoch 0004 | Loss 9.5046 | Best MRR 0.0000\n",
      "start eval\n",
      "test triplet 0 / 104139\n",
      "test triplet 100 / 104139\n",
      "test triplet 200 / 104139\n",
      "test triplet 300 / 104139\n",
      "test triplet 400 / 104139\n",
      "test triplet 500 / 104139\n",
      "test triplet 600 / 104139\n",
      "test triplet 700 / 104139\n",
      "test triplet 800 / 104139\n",
      "test triplet 900 / 104139\n",
      "test triplet 1000 / 104139\n",
      "test triplet 1100 / 104139\n",
      "test triplet 1200 / 104139\n",
      "test triplet 1300 / 104139\n",
      "test triplet 1400 / 104139\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [143]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart eval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m embed \u001b[38;5;241m=\u001b[39m model(test_g, test_nids)\n\u001b[0;32m---> 56\u001b[0m mrr \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_mrr\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw_relation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtriplets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m               \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfiltered\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# save best model\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_mrr \u001b[38;5;241m<\u001b[39m mrr:\n",
      "Input \u001b[0;32mIn [142]\u001b[0m, in \u001b[0;36mcalc_mrr\u001b[0;34m(emb, w, test_mask, triplets, batch_size, eval_p)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalc_mrr\u001b[39m(emb, w, test_mask, triplets, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, eval_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiltered\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_p \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiltered\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 30\u001b[0m         mrr \u001b[38;5;241m=\u001b[39m \u001b[43m_calc_mrr\u001b[49m\u001b[43m(\u001b[49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtriplets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m         mrr \u001b[38;5;241m=\u001b[39m _calc_mrr(emb, w, test_mask, triplets, batch_size)\n",
      "Input \u001b[0;32mIn [142]\u001b[0m, in \u001b[0;36m_calc_mrr\u001b[0;34m(emb, w, test_mask, triplets_to_filter, batch_size, filter)\u001b[0m\n\u001b[1;32m      9\u001b[0m     metric_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMRR (filtered)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m     triplets_to_filter \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mtuple\u001b[39m(triplet) \u001b[38;5;28;01mfor\u001b[39;00m triplet \u001b[38;5;129;01min\u001b[39;00m triplets_to_filter\u001b[38;5;241m.\u001b[39mtolist()}\n\u001b[0;32m---> 11\u001b[0m     ranks_s \u001b[38;5;241m=\u001b[39m \u001b[43mperturb_and_get_filtered_rank\u001b[49m\u001b[43m(\u001b[49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mtriplets_to_filter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_o\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     ranks_o \u001b[38;5;241m=\u001b[39m perturb_and_get_filtered_rank(emb, w, s, r, o,\n\u001b[1;32m     14\u001b[0m                                             test_size, triplets_to_filter)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Input \u001b[0;32mIn [141]\u001b[0m, in \u001b[0;36mperturb_and_get_filtered_rank\u001b[0;34m(emb, w, s, r, o, test_size, triplets_to_filter, filter_o)\u001b[0m\n\u001b[1;32m     60\u001b[0m     emb_o \u001b[38;5;241m=\u001b[39m emb[candidate_nodes]\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 62\u001b[0m     emb_s \u001b[38;5;241m=\u001b[39m \u001b[43memb\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcandidate_nodes\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     63\u001b[0m     emb_o \u001b[38;5;241m=\u001b[39m emb[target_o]\n\u001b[1;32m     64\u001b[0m target_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# The actual main\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training with RGCN module\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    graph = test_homograph # graph\n",
    "    num_nodes = graph.num_nodes()\n",
    "    num_rels = len(hetero_graph.etypes) # the same as data.num_rels\n",
    "\n",
    "    train_g = get_subset_g(graph, graph.edata[\"train_mask\"], num_rels)\n",
    "    test_g = get_subset_g(graph, graph.edata[\"train_mask\"], num_rels, bidirected=True)\n",
    "    test_g.edata[\"norm\"] = dgl.norm_by_dst(test_g).unsqueeze(-1)\n",
    "    test_nids = th.arange(0, num_nodes)\n",
    "    test_mask = graph.edata['test_mask']\n",
    "    subg_iter = SubgraphIterator(train_g, num_rels, pos_sampler='uniform')\n",
    "    dataloader = GraphDataLoader(subg_iter, batch_size=1, collate_fn=lambda x: x[0])\n",
    "    \n",
    "    # Prepare data for metric computation\n",
    "    src, dst = graph.edges()\n",
    "    triplets = th.stack([src, graph.edata['etype'], dst], dim=1)\n",
    "\n",
    "    model = LinkPredict(num_nodes, num_rels)\n",
    "    optimizer = th.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "    \n",
    "\n",
    "    best_mrr = 0\n",
    "    test_batch_size = 5 # set to be small for testing, suggest to be around 500 for actual training\n",
    "    model_state_file = 'model_state.pth'\n",
    "    for epoch, batch_data in enumerate(dataloader):\n",
    "        model.train()\n",
    "\n",
    "        g, train_nids, edges, labels = batch_data\n",
    "        g = g.to(device)\n",
    "        train_nids = train_nids.to(device)\n",
    "        edges = edges.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        embed = model(g, train_nids)\n",
    "        loss = model.get_loss(embed, edges, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # clip gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        print(\"Epoch {:04d} | Loss {:.4f} | Best MRR {:.4f}\".format(epoch, loss.item(), best_mrr))\n",
    "\n",
    "        if (epoch + 1) % test_batch_size == 0:\n",
    "            # perform validation on CPU because full graph is too large\n",
    "            model = model.cpu()\n",
    "            model.eval()\n",
    "            print(\"start eval\")\n",
    "            embed = model(test_g, test_nids)\n",
    "            mrr = calc_mrr(embed, model.w_relation, test_mask, triplets,\n",
    "                           batch_size=test_batch_size, eval_p='filtered')\n",
    "            # save best model\n",
    "            if best_mrr < mrr:\n",
    "                best_mrr = mrr\n",
    "                th.save({'state_dict': model.state_dict(), 'epoch': epoch}, model_state_file)\n",
    "\n",
    "            model = model.to(device)\n",
    "\n",
    "    print(\"Start testing:\")\n",
    "    # use best model checkpoint\n",
    "    checkpoint = th.load(model_state_file)\n",
    "    model = model.cpu() # test on CPU\n",
    "    model.eval()\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    print(\"Using best epoch: {}\".format(checkpoint['epoch']))\n",
    "    embed = model(test_g, test_nids)\n",
    "    calc_mrr(embed, model.w_relation, test_mask, triplets,\n",
    "             batch_size=test_batch_size, eval_p=args.eval_protocol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404bf349",
   "metadata": {},
   "source": [
    "#### Reference:\n",
    "Schlichtkrull, M., Kipf, T. N., Bloem, P., van den Berg, R., Titov, I., & Welling, M. (2018). Modeling relational data with graph convolutional networks. In European Semantic Web Conference (pp. 593-607). Springer, Cham. https://doi.org/10.1007/978-3-319-93417-4_38"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
